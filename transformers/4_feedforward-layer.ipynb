{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380478ca-79e3-49a0-8676-ee8cf2dea112",
   "metadata": {},
   "source": [
    "# BERT's Anatomy Step by Step: The Feedforward layer\n",
    "\n",
    "The feedforward layer in the encoder of a transformer model is a key component responsible for processing the information obtained from the self-attention mechanism.\n",
    "\n",
    "It consist of two fully connected (dense) layers with a GELU activation function in between. The feedforward layer operates independently on each position in the input sequence. The first linear transformation projects the high-dimensional representations learned by the self-attention mechanism into a lower-dimensional space, introducing non-linearity through the activation function. The second linear transformation then restores the dimensionality of the data. This configuration allows the feedforward layer to capture complex patterns and relationships within the input data, promoting the model's capacity to learn intricate features. The feedforward layer significantly contributes to the transformer's ability to model sequential dependencies and hierarchies, making it a vital element in achieving state-of-the-art performance in natural language processing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef9a343-af66-40c2-bbb2-ae0473c41e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from transformers import AutoConfig, AutoTokenizer\n",
    "from transformers import BertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd3506-1e3c-4cf0-8663-e000df0a6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6d8daa-9f35-485e-b07b-c8e62060640d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertForPreTraining.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f9dd74-738a-490e-9dd1-3fb138bfd2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fe5331-a169-4b77-b19e-11faf31c6353",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = AutoConfig.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87b5c19-faa4-47e5-bb34-f4d607ba26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"let's tokenize something?\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5e4ef01-b588-4750-b3a7-82cb25f296b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_embedding = model.bert.embeddings.word_embeddings(encoding)\n",
    "seq_embedding.shape   # [batch_size, seq_len, hidden_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2988d68-3fd5-41bf-8078-5f21328e96e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(seq_embedding):\n",
    "    head_dim = config.hidden_size // config.num_attention_heads\n",
    "    query = nn.Linear(config.hidden_size, head_dim)(seq_embedding)\n",
    "    key = nn.Linear(config.hidden_size, head_dim)(seq_embedding)\n",
    "    value = nn.Linear(config.hidden_size, head_dim)(seq_embedding)\n",
    "    dim_k = query.size(-1)\n",
    "    scores = torch.bmm(query, key.transpose(1, 2)) / torch.math.sqrt(dim_k)\n",
    "    weights = F.softmax(scores, dim=-1)\n",
    "    return torch.bmm(weights, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20fa261f-afc9-4e76-a7cb-070f4f886d73",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_concat = torch.cat([scaled_dot_product_attention(seq_embedding) for i in range(12)], dim=-1)\n",
    "att_concat.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38aab79f-9452-4ff6-808f-0a6c4f101b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_ouput = nn.Linear(config.hidden_size, config.hidden_size)(att_concat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336c2dd5-a603-479c-b413-a851ff62dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "att_ouput.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac100f92-1492-4916-a158-2dac935614a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.linear_1 = nn.Linear(config.hidden_size, config.intermediate_size)\n",
    "        self.linear_2 = nn.Linear(config.intermediate_size, config.hidden_size)\n",
    "        self.gelu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear_1(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.linear_2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59710f7-da40-4348-8260-c0c761c2f022",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedforward = FeedForward(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e6edb52-4645-480d-9d2d-c1503b610511",
   "metadata": {},
   "outputs": [],
   "source": [
    "feedforward(att_ouput).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0eb070-145e-435d-93c6-f4da7b3303bc",
   "metadata": {},
   "source": [
    "The feedforward layer operates independently on each position in the input sequence.\n",
    "In\n",
    "\n",
    "Note that a feed-forward layer such as nn.Linear is usually applied to a tensor of shape (batch_size, input_dim), where it acts on each element of the batch dimension independently. This is actually true for any dimension except the last one, so when we pass a tensor of shape (batch_size, seq_len, hidden_dim) the layer is applied to all token embeddings of the batch and sequence independently, which is exactly what we want. Letâ€™s test this by passing the attention outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac218d7-d214-4c05-b091-d5760b9a5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "_batch = torch.rand((64, config.hidden_size))\n",
    "feedforward(_batch).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ecd848c-874e-46d6-80cb-e727e233aa6c",
   "metadata": {},
   "source": [
    "## Layer normalization"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-2.3.0",
   "language": "python",
   "name": "pytorch-2.3.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
