{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10ed091b-e0c4-44f7-a41e-9c9e1c6f25c4",
   "metadata": {},
   "source": [
    "# The Multilayer Perceptron\n",
    "\n",
    "In deep learning, one of the fundamental architectures is the multilayer perceptron (MLP), serving as a cornerstone for more complex neural networks.\n",
    "\n",
    "At its core, a MLP consists of multiple layers of neurons, each layer connected to the next in a feedforward manner. These layers typically include an input layer, one or more hidden layers, and an output layer. The magic lies in the interconnectedness of these layers and the activation functions applied at each step.\n",
    "\n",
    "The following image shows a schematic representation of an MLP for classification tasks where models typically have a softmax activation function in the output layer, which outputs probabilities for each class\n",
    "<div style=\"text-align:center;\">\n",
    "    <img src=\"imgs/mlp-regression.svg\" alt=\"MLP for regression\" width=\"600\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d874c513-6970-44ad-bce4-66555ae31a3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c80ff4-1be8-4816-88e3-851cedd139a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch of inputs\n",
    "x = torch.randn((8, 784))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087d1cd8-7c2e-4688-9846-dfb782a56e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# An example of MLP for a regression problem\n",
    "class MLPRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)   # same size of the input vector\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(64, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)          # Apply Fully Connected layer 1   (batch_size, 784)    -> (batch_size, 128)\n",
    "        x = self.relu1(x)        # Apply ReLU activation Function  (batch_size, 784)\n",
    "        x = self.fc2(x)          # Apply Fully Connected layer 2   (batch_size, 128)    -> (batch_size, 64)\n",
    "        x = self.relu2(x)        # Apply ReLU activation Function  (batch_size, 64)\n",
    "        x = self.fc3(x)          # Apply Fully Connected layer 3   (batch_size, 64)     -> (batch_size, 10)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d5cd26-d594-4d47-b09f-55568c6477b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MLPRegression()\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7101c71-00bb-4f1e-9dd8-e2cd2f68c39e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca2682c1-976c-4ff6-801c-1ed74fbfd697",
   "metadata": {},
   "source": [
    "### Operations in the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7af31f-cf25-4d8b-a925-c5d8d13860b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply Fully Connected layer 1   (batch_size, 784)   -> (batch_size, 128)\n",
    "lin = nn.Linear(784, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf42eb76-e861-4610-b917-cdf77a8bea9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin(x).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087f7a0c-9bab-4e02-b070-f5c03b271d2e",
   "metadata": {},
   "source": [
    "Internally linear layers have two sets of parameters: the weights matrix $W$ and the biasses $\\vec{b}$. Applying a linear layer means doing a linear transformation $\\vec{x}^* = W\\vec{x} + \\vec{b}$.\n",
    "\n",
    "We can access the internals of the layer as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1aff50-4f64-4e78-aece-2bbf989a9e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f325ce37-bd0a-448b-8afa-90519a4abad8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin.bias.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c9482d-25da-425d-99b3-4bb639c13cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the linear transformation `x^* = Wx + b` by hand\n",
    "# for one sample in the batch: x_flat[0]\n",
    "#\n",
    "x_star = x[0] @ lin.weight.T + lin.bias\n",
    "\n",
    "x_star.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a49941a5-a71a-4acc-aacd-592e7fd34588",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparing with the linear layer\n",
    "torch.allclose(lin(x[0]), x_star) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2024",
   "language": "python",
   "name": "ml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
