{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "695ab70a-ffb5-42a0-b8c7-8e8509e3e909",
   "metadata": {},
   "source": [
    "# Basic training of a CNN on imagenet from tfrecord files using TensorFlow's `tf.data` API\n",
    "\n",
    "Here we will run a simplified training loop for a CNN model on ImageNet. We will create a TensorFlow's [`tf.data` API](https://www.tensorflow.org/guide/data) input pipeline based to feed to model with ImageNet data stored in tfrecord files. We will apply random transformations to the images as done [here](https://www.tensorflow.org/tutorials/images/data_augmentation#apply_augmentation_to_a_dataset).\n",
    "\n",
    "We use [TensorFlow Datasets](https://www.tensorflow.org/datasets) to convert a `tf.data.Dataset` dataset to an iterable of NumPy arrays:\n",
    "```python\n",
    "np_dataset = tfds.as_numpy(tf_dataset)\n",
    "```\n",
    "from which the data is converted to `torch.tensor` and then moved to the GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2363679-29a4-402d-bc2c-9cd31fe0944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import tensorflow_datasets as tfds\n",
    "import torch.optim as optim\n",
    "import tensorflow as tf\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab170ac-62d3-460d-bf88-aa4da966cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfrec_files = glob.glob(f'/scratch/snx3000/datasets/imagenet/ILSVRC2012_1k//train/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98464f6a-c2ad-46b4-8a93-7dca2ac1660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.set_visible_devices(\n",
    "    tf.config.list_physical_devices('CPU')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3510ac1a-ede0-4718-9c03-bd8a6701a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe08a9-59a5-4fdb-b384-79f982319de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(serialized_example):\n",
    "    \"\"\"Decode and resize\"\"\"\n",
    "    example = tf.io.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        })\n",
    "    image = tf.image.decode_jpeg(example['image/encoded'], channels=3)\n",
    "    label = example['image/class/label'] - 1  # -> [0-999]\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c8c04f-9f8a-468c-b842-dc2be146e961",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "\n",
    "def resize_and_rescale(image, label):\n",
    "    image = tf.cast(image, tf.float32)\n",
    "    image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "    image = (image / 255.0)\n",
    "    return image, label\n",
    "\n",
    "def _augment(image, label, seed):\n",
    "    # image, label = image_label\n",
    "    image, label = resize_and_rescale(image, label)\n",
    "    image = tf.image.resize_with_crop_or_pad(image, IMG_SIZE + 6, IMG_SIZE + 6)\n",
    "    # Make a new seed.\n",
    "    new_seed = tf.random.experimental.stateless_split(seed, num=1)[0, :]\n",
    "    # Random crop back to the original size.\n",
    "    image = tf.image.stateless_random_crop(\n",
    "      image, size=[IMG_SIZE, IMG_SIZE, 3], seed=seed)\n",
    "    # Random brightness.\n",
    "    image = tf.image.stateless_random_brightness(\n",
    "      image, max_delta=0.5, seed=new_seed)\n",
    "    image = tf.clip_by_value(image, 0, 1)\n",
    "    image = tf.transpose(image, (2, 0, 1))\n",
    "    return image, label\n",
    "\n",
    "# Create a generator.\n",
    "rng = tf.random.Generator.from_seed(123, alg='philox')\n",
    "\n",
    "# Create a wrapper function for updating seeds.\n",
    "def augment(x, y):\n",
    "    seed = rng.make_seeds(2)[0]\n",
    "    image, label = _augment(x, y, seed)\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0b11b-4c01-4514-aa6e-945636da78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = tf.data.TFRecordDataset(tfrec_files)\n",
    "dataset = dataset.map(decode, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.map(augment, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b97283-e7a1-4a4f-a996-d78e82eb5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_np = tfds.as_numpy(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1a310-7d59-44ea-be87-926f318c983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0\n",
    "\n",
    "model = models.resnet50()\n",
    "model.to(device);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8706059-a638-4416-98dd-be9f91f0003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55873c-a2ac-4e31-b0ec-f7ef4d8ad4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_step(model, imgs, labels):\n",
    "    optimizer.zero_grad()\n",
    "    output = model(imgs)\n",
    "    loss = F.cross_entropy(output, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51853a-dffa-437d-9656-d739429820a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5\n",
    "num_iters = 10\n",
    "imgs_sec = []\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    for step, (imgs, labels) in enumerate(dataset_np):\n",
    "        if step > num_iters:\n",
    "            break\n",
    "\n",
    "        imgs = torch.from_numpy(imgs).to(device)\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "        benchmark_step(model, imgs, labels)\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    imgs_sec.append(batch_size * num_iters / dt)\n",
    "\n",
    "    print(f' * Epoch {epoch:2d}: '\n",
    "          f'{imgs_sec[epoch]:.2f} images/sec per GPU')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2022",
   "language": "python",
   "name": "pytorch2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
