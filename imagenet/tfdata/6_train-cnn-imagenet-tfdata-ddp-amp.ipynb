{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37a4f276-6634-4387-8b5b-6704d9500888",
   "metadata": {},
   "source": [
    "# Basic training on multiple GPUs of a CNN on imagenet from tfrecord files using TensorFlow's `tf.data` API\n",
    "\n",
    "Here we will run a simplified training loop for a CNN model on ImageNet. We will create a TensorFlow's [`tf.data` API](https://www.tensorflow.org/guide/data) input pipeline based to feed to model with ImageNet data stored in tfrecord files.\n",
    "\n",
    "We use [TensorFlow Datasets](https://www.tensorflow.org/datasets) to convert a `tf.data.Dataset` dataset to an iterable of NumPy arrays:\n",
    "```python\n",
    "np_dataset = tfds.as_numpy(tf_dataset)\n",
    "```\n",
    "from which the data is converted to `torch.tensor` and then moved to the GPU.\n",
    "\n",
    "In this notebook we will use PyTorch's [Automatic Mixed Precision](https://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#all-together-automatic-mixed-precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd54a174-888a-49b3-995a-9c7adf97872a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipcmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d8e78c-1608-4118-96ca-0f3e62469e7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ipcluster start -n 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e943cb-4442-4ebd-839a-e99968167109",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pxconfig --progress-after -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2363679-29a4-402d-bc2c-9cd31fe0944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "import glob\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.distributed as dist\n",
    "import tensorflow as tf\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "from torchvision import models\n",
    "from pt_distr_env import DistributedEnviron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab170ac-62d3-460d-bf88-aa4da966cda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "tfrec_files = glob.glob(f'/scratch/snx3000/datasets/imagenet/ILSVRC2012_1k//train/*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98464f6a-c2ad-46b4-8a93-7dca2ac1660e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "tf.config.set_visible_devices(\n",
    "    tf.config.list_physical_devices('CPU')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725b6266-cbf2-469f-ba5c-68fe664353a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "distr_env = DistributedEnviron()\n",
    "dist.init_process_group(backend=\"nccl\")\n",
    "world_size = dist.get_world_size()\n",
    "rank = dist.get_rank()\n",
    "device = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3510ac1a-ede0-4718-9c03-bd8a6701a6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe08a9-59a5-4fdb-b384-79f982319de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "def decode(serialized_example):\n",
    "    \"\"\"Decode and resize\"\"\"\n",
    "    example = tf.io.parse_single_example(\n",
    "        serialized_example,\n",
    "        features={\n",
    "            'image/encoded': tf.io.FixedLenFeature([], tf.string),\n",
    "            'image/class/label': tf.io.FixedLenFeature([], tf.int64),\n",
    "        })\n",
    "    image = tf.image.decode_jpeg(example['image/encoded'], channels=3)\n",
    "    image = tf.image.resize_with_crop_or_pad(image, 224, 224)\n",
    "    image = tf.transpose(image, (2, 0, 1)) # rgb channels to the front\n",
    "    image = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
    "    label = example['image/class/label'] - 1  # -> [0-999]\n",
    "    return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd0b11b-4c01-4514-aa6e-945636da78f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "dataset = tf.data.TFRecordDataset(tfrec_files)\n",
    "dataset = dataset.map(decode, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)\n",
    "dataset = dataset.shard(world_size, rank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b97283-e7a1-4a4f-a996-d78e82eb5419",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "dataset_np = tfds.as_numpy(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fd1a310-7d59-44ea-be87-926f318c983e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "_model = models.resnet50()\n",
    "_model.to(device);\n",
    "\n",
    "ddp_model = DistributedDataParallel(_model, device_ids=[device])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8706059-a638-4416-98dd-be9f91f0003e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "optimizer = optim.SGD(ddp_model.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da55873c-a2ac-4e31-b0ec-f7ef4d8ad4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "use_amp = True\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
    "\n",
    "def benchmark_step_amp(model, imgs, labels):\n",
    "    optimizer.zero_grad()\n",
    "    with torch.autocast(device_type='cuda',\n",
    "                        dtype=torch.float16,\n",
    "                        enabled=use_amp):\n",
    "        output = model(imgs)\n",
    "        loss = F.cross_entropy(output, labels)\n",
    "\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca51853a-dffa-437d-9656-d739429820a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%px\n",
    "print()\n",
    "num_epochs = 5\n",
    "num_iters = 10\n",
    "imgs_sec = []\n",
    "for epoch in range(num_epochs):\n",
    "    t0 = time.time()\n",
    "    for step, (imgs, labels) in enumerate(dataset_np):\n",
    "        if step > num_iters:\n",
    "            break\n",
    "\n",
    "        imgs = torch.from_numpy(imgs).to(device)\n",
    "        labels = torch.from_numpy(labels).to(device)\n",
    "        benchmark_step_amp(ddp_model, imgs, labels)\n",
    "\n",
    "    dt = time.time() - t0\n",
    "    imgs_sec.append(batch_size * num_iters / dt)\n",
    "\n",
    "    print(f' * Epoch {epoch:2d}: '\n",
    "          f'{imgs_sec[epoch]:.2f} images/sec per GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cbf036-f2ec-4656-9a71-d117949fef87",
   "metadata": {},
   "outputs": [],
   "source": [
    "%ipcluster stop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch2022",
   "language": "python",
   "name": "pytorch2022"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
