{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2aaf309d-516c-4dc9-9dd8-cded5cfe9b30",
   "metadata": {},
   "source": [
    "# Activation Functions\n",
    "\n",
    "Here we visualize different activation functions commonly used in neural networks.\n",
    "**Activation functions introduce non-linearity to the model**, enabling it to learn complex patterns and relationships within the data.\n",
    "\n",
    "Let's have a look to a few of them:\n",
    "1. **ReLU (Rectified Linear Unit)** is one of the most widely used activation functions in neural networks. It introduces non-linearity by simply thresholding the input at zero, setting all negative values to zero.\n",
    "2. **Leaky ReLU** is a variant of ReLU that addresses the \"dying ReLU\" problem (neurons always outputting zero for any input, becoming inactive). Instead of setting negative values to zero, Leaky ReLU allows a small, positive gradient for negative inputs.\n",
    "3. **GELU (Gaussian Error Linear Unit)** approximates the Gaussian Cumulative Distribution Function (CDF) and exhibits smoothness while retaining desirable properties of non-linear activations.\n",
    "4. **ELU (Exponential Linear Unit)** is another variant of ReLU that has a smooth non-linearity for negative inputs. It has an exponential behavior for negative values, which helps mitigate the vanishing gradient problem.\n",
    "5. **Tanh (Hyperbolic Tangent)** squashes the input values between -1 and 1, making it useful for models where the output needs to be normalized.\n",
    "6. **Sigmoid Logistic Sigmoid)** function compresses the input values between 0 and 1, making it suitable for binary classification tasks where the output needs to represent probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe5e418-ceb9-4a8b-9f91-8a78dcdca123",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'svg'\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from util import show_act_functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "befab68d-4a20-4d1c-a070-e948fd6b1681",
   "metadata": {},
   "outputs": [],
   "source": [
    "functions = [nn.ReLU, nn.LeakyReLU, nn.GELU, nn.ELU, nn.Tanh, nn.Sigmoid]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfddf43a-71ad-4695-a405-90a6f40e1270",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(-10, 10, 0.1)\n",
    "\n",
    "show_act_functions(x, functions, num_col=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758c1f7c-6d09-4161-91b6-c0a6759c1cb3",
   "metadata": {},
   "source": [
    "Let's apply an activation function to the output of a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cca1ad-c91f-49e4-8aa2-e67b5406079f",
   "metadata": {},
   "outputs": [],
   "source": [
    "lin = nn.Linear(784, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d81dce8-564e-40b4-9437-892a58edb6f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((8, 784))  # Fake data\n",
    "x = lin(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673d922a-267a-47b0-a8ed-c8fc4323e4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x[0].detach().numpy(), lw=1, label='Linear layer output')\n",
    "plt.plot(nn.ReLU()(x[0]).detach().numpy(), alpha=0.6, lw=2, label='Act function applied')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml2024",
   "language": "python",
   "name": "ml2024"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
