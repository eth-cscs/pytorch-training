{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of a Simple Stochastic Gradient Descent\n",
    "\n",
    "Here we visualize the minimization of the loss with the SGD algorithm in its variants vanilla GD, batch SGD and minibatch SGD. For this we consider a linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "Let's create a [PyTorch dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). In PyTorch, the `Dataset` object is an abstract class from `torch.utils.data` that we subclass to create out own custom dataset. It defines how to access individual data samples and how many samples there are."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearDataset(Dataset):\n",
    "    '''The training data is generated from the linear\n",
    "    function\n",
    "              y = w * x + v  \n",
    "    where `w` is the slope and `b` is the offset.\n",
    "    Random noise in the range `[-0.1, 0.1]` is added\n",
    "    to the function value `y`.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, slope, offset, nsamples):\n",
    "        self.w = slope\n",
    "        self.b = offset\n",
    "        self.nsamples = nsamples\n",
    "\n",
    "        # A dedicated random number generator for the dataset\n",
    "        self.rnd_gen_data = torch.Generator().manual_seed(1)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.FloatTensor(1,).uniform_(-0.5, 0.5, generator=self.rnd_gen_data)\n",
    "        y = self.w * x + self.b\n",
    "        noise = torch.FloatTensor(1,).uniform_(-0.1, 0.1, generator=self.rnd_gen_data)\n",
    "        return (x, y + noise)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.nsamples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate a random vector $x \\in [-0.5, 0.5]$ and evaluate it in a linear function $y = 2w$ (we set the offset $b$ to be 0).\n",
    "We add noise to $y$ and that gives us $y \\in [-1.1, 1.1]$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = LinearDataset(\n",
    "    slope = 2.0,\n",
    "    offset = 0.0,\n",
    "    nsamples = 1500\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_set,\n",
    "    batch_size=128,\n",
    "    shuffle=False,\n",
    "    drop_last=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check how the batches look like\n",
    "for x, y in train_loader:\n",
    "    print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_plot = []\n",
    "y_plot = []\n",
    "for x, y in train_loader:\n",
    "    x_plot.append(x)\n",
    "    y_plot.append(y)\n",
    "\n",
    "x_plot = torch.cat(x_plot)\n",
    "y_plot = torch.cat(y_plot)\n",
    "\n",
    "plt.plot(x_plot, y_plot, '.')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Defining the model, a loss function and an optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 0   # First GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a model: \n",
    "# Create a linear model with 1 input and 1 output (y = wx + b),\n",
    "# and move it to the selected device\n",
    "model = torch.nn.Linear(1, 1, device=device)\n",
    "\n",
    "# Select a loss function\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "\n",
    "# Select an optimizer\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log(model, loss):\n",
    "    \"\"\"Utility function for plotting\"\"\"\n",
    "\n",
    "    return(model.weight.item(),\n",
    "           model.bias.item(),\n",
    "           loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25\n",
    "history = []\n",
    "\n",
    "for epoch in range(num_epochs):                 #   loop over epochs:\n",
    "    # set a seed at each epoch to always        #\n",
    "    # generate the same set of random x         #\n",
    "    train_set.rnd_gen_data.manual_seed(1)       #\n",
    "    #                                           #\n",
    "    for x, y in train_loader:                   #      loop over batches:  -> (x, y)\n",
    "        optimizer.zero_grad()                   #         * reset automatic differentiation record\n",
    "        y_hat = model(x.to(device))             #         * evaluate the model in a batch -> y_hat (forward pass)\n",
    "        loss = loss_fn(y_hat, y.to(device))     #         * evaluate the loss function with the obtained y_hat and y\n",
    "        history.append(log(model, loss))        #         [not part of the traing] keep values for plotting later\n",
    "        loss.backward()                         #         * backpropagation -> gradients\n",
    "        optimizer.step()                        #         * update weights with the gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "slope_hist  = np.array(history)[:, 0]\n",
    "offset_hist = np.array(history)[:, 1]\n",
    "loss_hist   = np.array(history)[:, 2]\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (10, 4)\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(loss_hist, 'r-')\n",
    "plt.xlabel('Training steps')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(x_plot, y_plot, '.', alpha=.1)\n",
    "plt.plot(x_plot, slope_hist[0]  * x_plot + offset_hist[0],  'r-', label='model (initial step)', lw=3)\n",
    "plt.plot(x_plot, slope_hist[-1] * x_plot + offset_hist[-1], 'b-', label='model (trained)')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function_field(m, n, xref, yref):\n",
    "    '''Utility function for ploting the loss'''\n",
    "    return np.mean(np.square(yref - m * xref - n ))\n",
    "\n",
    "_m = np.arange( -0.5, 4.51, 0.1)\n",
    "_n = np.arange(-1., 1.01, 0.1)\n",
    "M, N = np.meshgrid(_m, _n)\n",
    "\n",
    "Z = np.zeros(M.shape)\n",
    "for i in range(M.shape[0]):\n",
    "    for j in range(M.shape[1]):\n",
    "        Z[i, j] = loss_function_field(M[i, j], N[i, j],\n",
    "                                      x_plot.numpy(), y_plot.numpy())\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (16.0, 5.2)\n",
    "\n",
    "cp = plt.contour(M, N, Z, 16, vmin=Z.min(), vmax=Z.max(), alpha=0.99, colors='k', linestyles=':', linewidths=0.8)\n",
    "plt.contourf(M, N, Z, 60, vmin=Z.min(), vmax=Z.max(), alpha=0.2, cmap='Blues')  #plt.cm.RdYlBu_r)\n",
    "plt.clabel(cp, cp.levels[:6])\n",
    "plt.colorbar()\n",
    "m = slope_hist[-1]\n",
    "n = offset_hist[-1]\n",
    "plt.plot([train_set.w], [train_set.b], 'rx', ms=10)\n",
    "plt.plot(slope_hist, offset_hist, '.-', lw=2, c='k')\n",
    "plt.xlim([_m.min(), _m.max()])\n",
    "plt.ylim([_n.min(), _n.max()])\n",
    "plt.xlabel('w')\n",
    "plt.ylabel('b')\n",
    "plt.show()\n",
    "\n",
    "matplotlib.rcParams['figure.figsize'] = (6, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "1. Try different batch sizes and see the effect path of the SGD:\n",
    " * `batch_size = 1000` - The whole dataset\n",
    " * `batch_size = 128`  - Something between 1 and the size of the whole dataset\n",
    " * `batch_size = 1`    - This is perhaps too slow. Maybe try a smaller batch size, such as 16\n",
    "\n",
    "2. Why is the training with `batch_size = 1` so slow compared to `batch_size = 128`?\n",
    "3. Try different learning rates and see how the path of the SGD looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Technical check\n",
    "\n",
    "By this point, you should be comfortable with the following:\n",
    " * Explaining what a batch is and why batching is important in training neural networks.\n",
    " * Explaining what's the learning rate and it's eefect on the training\n",
    " * Implementing a custom Dataset and wraping it in a DataLoader using PyTorch.\n",
    " * Transfering data to GPU memory using `.to(device)` for faster computation.\n",
    " * Instantiating a model, defining a loss function, and setting up an optimizer in PyTorch.\n",
    " * Writing a full training loop that iterates over batches, computes loss, and updates model parameters."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
