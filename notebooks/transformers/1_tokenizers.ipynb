{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "380478ca-79e3-49a0-8676-ee8cf2dea112",
   "metadata": {},
   "source": [
    "# BERT's Tokenizer\n",
    "\n",
    "Tokenizers are essential tools in natural language processing for breaking down text into smaller units, such as words or subwords, known as tokens. These tokens serve as the fundamental building blocks for various NLP tasks, such as text classification, sentiment analysis, and machine translation.\n",
    "\n",
    "Here, we will have a look to the BERT tokenizer implemented in [HuggingFace](https://huggingface.co). Although not part of the model, tokenizers are a crucial component of language models, in this case BERT (Bidirectional Encoder Representations from Transformers). BERT uses the [WordPiece](https://huggingface.co/docs/transformers/en/tokenizer_summary#wordpiece) tokenization which segments words into meaningful subword units, allowing for a comprehensive understanding of context and semantics within a given text corpus. Through this exploration, we aim to understand the intricacies of tokenization and its significance in modern NLP frameworks\n",
    "\n",
    " * [[huggingface.co] Summary of the tokenizers](https://huggingface.co/docs/transformers/en/tokenizer_summary)\n",
    " * [[youtube] **Let's build the GPT Tokenizer** by Andrej Karpathy](https://www.youtube.com/watch?v=zduSFxRajkE)\n",
    " * [[huggingface.co] Byte-Pair Encoding tokenization](https://huggingface.co/learn/nlp-course/en/chapter6/5)\n",
    " * [Tiktoken App](https://tiktokenizer.vercel.app/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef9a343-af66-40c2-bbb2-ae0473c41e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40cd3506-1e3c-4cf0-8663-e000df0a6865",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'bert-base-uncased'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f9dd74-738a-490e-9dd1-3fb138bfd2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe9fcaf-b10e-448a-95bc-b04ed36b812f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593ea66-ccd5-45a3-b005-7034f830e29f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tokenizer.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87b5c19-faa4-47e5-bb34-f4d607ba26fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoding = tokenizer.encode(\"let's tokenize something?\", return_tensors=\"pt\")\n",
    "encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee421f5c-0ea6-4f77-b15d-82a075a1f9cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(encoding.flatten())\n",
    "tokens"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
